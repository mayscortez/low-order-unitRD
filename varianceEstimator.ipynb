{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variance Estimator\n",
    "Let $P_\\mathbf{x} := \\mathbb{P}(\\mathbf{z} = \\mathbf{x})$.\n",
    "Let $\\mathbf{z}^*$ be the realized treatment vector.\n",
    "Let $P^* := \\mathbb{P}(\\mathbf{z} = \\mathbf{z^*})$ and $P_S^* := \\mathbb{P}(\\mathbf{z_S} = \\mathbf{z_S^*})$ for $S\\subseteq[n]$.\n",
    "Then, the variance estimator is given by $\\widehat{\\text{Var}}(\\widehat{TTE}) = (*) + (**)$ where \n",
    "$$(*) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j \\in M_i} \\frac{1}{P_{N_i \\cup N_j}^*} \\cdot Y_i(\\mathbf{z}^*)w_i(\\mathbf{z}^*) \\cdot Y_j(\\mathbf{z}^*)w_j(\\mathbf{z}^*)\\cdot \\text{Cov}_{ij}$$\n",
    "and\n",
    "$$(**) = \\frac{1}{n^2} \\sum_{i=1}^n P_{N_i}^* \\cdot Y_i^2\\mathbf{z^*} w_i^2\\mathbf{z^*} \\cdot \\sum_{j \\in M_i} 2^{|N_j|} - 2^{|N_j\\setminus N_i|}$$\n",
    "with \n",
    "$$\\text{Cov}_{ij} := \\prod_{k \\in N_i \\cup N_j} p^{\\mathbf{z}_k^*}(1-p)^{1-\\mathbf{z}_k^*}\\left(1 - \\prod_{\\ell \\in N_i \\cap N_j} p^{\\mathbf{z}_\\ell^*}(1-p)^{1-\\mathbf{z}_\\ell^*}\\right).$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Term 1\n",
    "We compute the first term, $(*)$, as follows:\n",
    "- First, create an $n \\times 1$ array $\\mathbf{YW}$ with entry $i$ equal to $Y_i(\\mathbf{z}^*)w_i(\\mathbf{z}^*)$.\n",
    "- Next, we create an $n \\times 1$ array $\\mathbf{V}$ with entry $i$ equal to $$\\sum_{j \\in M_i} \\frac{1}{P_{N_i \\cup N_j}^*} \\cdot Y_j(\\mathbf{z}^*)w_j(\\mathbf{z}^*)\\cdot \\text{Cov}_{ij}.$$\n",
    "    -- For each $i \\in [n]$, we create the following $m_i := |M_i| \\times 1$ arrays: \n",
    "    $$\\mathbf{P}_i := \\begin{bmatrix} 1/P_{N_i \\cup N_{j_1}} \\\\ \\vdots \\\\ 1/P_{N_i \\cup N_{j_{m_i}}} \\end{bmatrix},\\ \n",
    "    \\mathbf{YW}_i = \\begin{bmatrix} Y_{j_1}(\\mathbf{z}^*)w_{j_1}(\\mathbf{z}^*) \\\\ \\vdots \\\\ Y_{j_{m_i}}(\\mathbf{z}^*)w_{j_{m_i}}(\\mathbf{z}^*) \\end{bmatrix}, \\\n",
    "    \\textbf{COV}_i := \\begin{bmatrix} \\text{Cov}_{i{j_1}} \\\\ \\vdots \\\\ \\text{Cov}_{i{j_{m_i}}} \\end{bmatrix}.$$\n",
    "\n",
    "    -- Then, entry-wise multiply these arrays to get one array, sum the entries of the resulting array, and that sum is the $i$-th entry of $\\mathbf{V}$.\n",
    "- Then, return the dot product of $\\mathbf{YW}$ and $\\mathbf{V}$ divided by $n^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Term 2\n",
    "We compute the second term, $(**)$, as follows:\n",
    "- First, compute an $n \\times 1$ array $\\mathbf{PY2W2}$ where entry $i$ equals $P_{N_i}^* Y_i^2(\\mathbf{z}^*) w_i^2(\\mathbf{z}^*).$\n",
    "- Then, compute an $n \\times 1$ array $\\mathbf{CNTS}$ where entry $i$ equals $$\\sum_{j\\in M_i} 2^{|N_j|} - 2^{|N_j \\setminus N_i|}.$$\n",
    "- Finally, take the dot product of $\\mathbf{PY2W2}$ and $\\mathbf{CNTS}$ and divide by $n^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scipy sparse A\n",
    "def var_est_log(n, p, y, A, z):    \n",
    "    YW = y * A.dot(z-p)/(p*(1-p))\n",
    "\n",
    "    P = np.power(np.ones(n)*p, z) + np.power(1 - np.ones(n)*p, 1 - z) - 1    # assignment probabilities\n",
    "    lP = np.log(P)     # log of assignment probabilities\n",
    "\n",
    "    PY2W2 = np.exp(A.dot(lP)) * YW**2 \n",
    "    \n",
    "    N = [np.nonzero(A[[i],:])[1] for i in range(n)]  # neighbors\n",
    "  \n",
    "    dep_neighbors = A.dot(A.transpose())\n",
    "    M = [np.nonzero(dep_neighbors[[i],:])[1] for i in range(n)] # dependencies\n",
    "\n",
    "    T1,T2 = 0,0\n",
    "    for i in range(n):\n",
    "        T1 += YW[i] * YW[M[i]].dot(1 - np.exp((A[[i],:]*A[M[i],:]).dot(lP)))\n",
    "        T2 += PY2W2[i] * sum([2**len(N[j]) - 2**len(np.setdiff1d(N[j],N[i])) for j in M[i]])\n",
    "\n",
    "    return (T1+T2)/n**2\n",
    "\n",
    "# for scipy sparse A\n",
    "def var_est_log2(n, p, y, A, z, N, M):    \n",
    "    YW = y * A.dot(z-p)/(p*(1-p))\n",
    "\n",
    "    P = np.power(np.ones(n)*p, z) + np.power(1 - np.ones(n)*p, 1 - z) - 1    # assignment probabilities\n",
    "    lP = np.log(P)     # log of assignment probabilities\n",
    "\n",
    "    PY2W2 = np.exp(A.dot(lP)) * YW**2 \n",
    "\n",
    "    T1,T2 = 0,0\n",
    "    for i in range(n):\n",
    "        T1 += YW[i] * YW[M[i]].dot(1 - np.exp((A[[i],:]*A[M[i],:]).dot(lP)))\n",
    "        T2 += PY2W2[i] * sum([2**len(N[j]) - 2**len(np.setdiff1d(N[j],N[i])) for j in M[i]])\n",
    "\n",
    "    return (T1+T2)/n**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scipy sparse A\n",
    "def var_est_sp(n, p, y, A, z):\n",
    "    '''\n",
    "    n : int\n",
    "        size of the population\n",
    "    p : float\n",
    "        treatment probability\n",
    "    y : numpy array\n",
    "        observations\n",
    "    A : scipy SPARSE matrix \n",
    "        adjacency matrix where (i,j)th entry = 1 iff j's treatment affects i's outcome\n",
    "    z : numpy array\n",
    "        realized treatment assignment vector\n",
    "    '''\n",
    "    zz = z/p - (1-z)/(1-p)\n",
    "    w = A.dot(zz)\n",
    "    YW = y * w\n",
    "    YW_sq = np.square(YW)\n",
    "\n",
    "    V = np.zeros(n)\n",
    "    PY2W2 = np.zeros(n)\n",
    "    CNTS = np.zeros(n)\n",
    "\n",
    "    prob_p = np.power(np.ones(n)*p, z) \n",
    "    prob_1_minus_p = np.power(1 - np.ones(n)*p, 1 - z)\n",
    "\n",
    "    dep_neighbors = A.dot(A.transpose())\n",
    "    \n",
    "    for i in np.arange(n):\n",
    "        Ni = np.nonzero(A[[i],:])[1]\n",
    "        Mi = np.nonzero(dep_neighbors[[i],:])[1] # dependency neighbor's indices\n",
    "\n",
    "        Pi = np.zeros(len(Mi))\n",
    "        COVi = np.zeros(len(Mi))\n",
    "        sum = 0\n",
    "        for j in np.arange(len(Mi)):\n",
    "            Nj = np.nonzero(A[[Mi[j]], :])[1]\n",
    "            Ni_or_Nj = np.union1d(Ni,Nj)\n",
    "\n",
    "            # Compute Pi\n",
    "            mult_p = prob_p[Ni_or_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_or_Nj]\n",
    "            Pi[j] = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "\n",
    "            # Compute COVi\n",
    "            Ni_and_Nj = np.intersect1d(Ni,Nj)\n",
    "            mult_p = prob_p[Ni_and_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_and_Nj]\n",
    "            temp = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "            COVi[j] = Pi[j] * (1 - temp)\n",
    "\n",
    "            # Compute CNTS[i]\n",
    "            Nj_minus_Ni = np.setdiff1d(Nj,Ni)\n",
    "            sum = sum + (2**len(Nj) - 2**len(Nj_minus_Ni))\n",
    "\n",
    "\n",
    "        # Compute V\n",
    "        V[i] = np.sum(1/Pi * YW[Mi] * COVi)\n",
    "\n",
    "        # Compute PY2W2\n",
    "        mult_p = prob_p[Ni]\n",
    "        mult_1_minus_p = prob_1_minus_p[Ni]\n",
    "        PY2W2[i] = np.prod(mult_p) * np.prod(mult_1_minus_p) * YW_sq[i]\n",
    "\n",
    "        # Compute CNTS\n",
    "        CNTS[i] = sum\n",
    "    \n",
    "    term1 = (1/n**2) * np.dot(YW, V)\n",
    "    term2 = (1/n**2) * np.dot(PY2W2, CNTS)\n",
    "    return term1+term2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numpy array A\n",
    "def var_est_np(n, p, y, A, z):\n",
    "    '''\n",
    "    n : int\n",
    "        size of the population\n",
    "    p : float\n",
    "        treatment probability\n",
    "    y : numpy array\n",
    "        observations\n",
    "    A : numpy matrix \n",
    "        adjacency matrix where (i,j)th entry = 1 iff j's treatment affects i's outcome\n",
    "    z : numpy array\n",
    "        realized treatment assignment vector\n",
    "    '''\n",
    "    zz = z/p - (1-z)/(1-p)\n",
    "    w = A.dot(zz)\n",
    "    YW = y * w\n",
    "    YW_sq = np.square(YW)\n",
    "\n",
    "    V = np.zeros(n)\n",
    "    PY2W2 = np.zeros(n)\n",
    "    CNTS = np.zeros(n)\n",
    "\n",
    "    prob_p = np.power(np.ones(n)*p, z) \n",
    "    prob_1_minus_p = np.power(1 - np.ones(n)*p, 1 - z)\n",
    "\n",
    "    dep_neighbors = np.dot(A,A.T)\n",
    "    \n",
    "    for i in np.arange(n):\n",
    "        Ni = np.nonzero(A[[i],:])[1]\n",
    "        Mi = np.nonzero(dep_neighbors[[i],:])[1] # shares an in-neighbor\n",
    "        \n",
    "        Pi = np.zeros(len(Mi))\n",
    "        COVi = np.zeros(len(Mi))\n",
    "        sum = 0\n",
    "        for j in np.arange(len(Mi)):\n",
    "            Nj = np.nonzero(A[[Mi[j]], :])[1]\n",
    "            Ni_or_Nj = np.union1d(Ni,Nj)\n",
    "\n",
    "            # Compute Pi\n",
    "            mult_p = prob_p[Ni_or_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_or_Nj]\n",
    "            Pi[j] = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "\n",
    "            # Compute COVi\n",
    "            Ni_and_Nj = np.intersect1d(Ni,Nj)\n",
    "            mult_p = prob_p[Ni_and_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_and_Nj]\n",
    "            temp = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "            COVi[j] = Pi[j] * (1 - temp)\n",
    "\n",
    "            # Compute CNTS[i]\n",
    "            Nj_minus_Ni = np.setdiff1d(Nj,Ni)\n",
    "            sum = sum + (2**len(Nj) - 2**len(Nj_minus_Ni))\n",
    "\n",
    "\n",
    "        # Compute V\n",
    "        V[i] = np.sum(1/Pi * YW[Mi] * COVi)\n",
    "\n",
    "        # Compute PY2W2\n",
    "        mult_p = prob_p[Ni]\n",
    "        mult_1_minus_p = prob_1_minus_p[Ni]\n",
    "        PY2W2[i] = np.prod(mult_p) * np.prod(mult_1_minus_p) * YW_sq[i]\n",
    "\n",
    "        # Compute CNTS\n",
    "        CNTS[i] = sum\n",
    "    \n",
    "    #print(\"V: {}\".format(V))\n",
    "    #print(\"PY2W2: {}\".format(PY2W2))\n",
    "    #print(\"CNTS: {}\\n\".format(CNTS))\n",
    "    term1 = (1/n**2) * np.dot(YW, V)\n",
    "    #print(\"term1: {}\".format(term1))\n",
    "    term2 = (1/n**2) * np.dot(PY2W2, CNTS)\n",
    "    #print(\"term2: {}\".format(term2))\n",
    "    return term1+term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_bound(n, p, A, C, alp, beta=1):\n",
    "    '''\n",
    "    Returns the conservative upper bound on the variance of the SNIPE(beta) estimator\n",
    "\n",
    "    n (int): size of the population\n",
    "    p (float): treatment probability\n",
    "    A (scipy sparse array): adjacency matrix\n",
    "    C (scipy sparse array): weighted adjacency matrix\n",
    "    alp (numpy array): baseline effects\n",
    "    beta (int): degree of the potential outcomes model\n",
    "    '''\n",
    "    in_deg = scipy.sparse.diags(np.array(A.sum(axis=1)).flatten(),0)  # array of the in-degree of each node\n",
    "    out_deg = scipy.sparse.diags(np.array(A.sum(axis=0)).flatten(),0)  # array of the out-degree of each node\n",
    "    in_deg = in_deg.tocsr() \n",
    "    out_deg = out_deg.tocsr() \n",
    "\n",
    "    d_in = in_deg.max()\n",
    "    d_out = out_deg.max()\n",
    "    temp = max(4 * (beta**2), (1 / (p*(1-p))))\n",
    "\n",
    "    if beta == 1:\n",
    "        Ymax = np.amax(scipy.sparse.diags(np.array(C.sum(axis=1)).flatten(),0) + alp)\n",
    "    else:\n",
    "        Ymax = np.amax(1 + alp)\n",
    "\n",
    "    bound = (1/n) * d_in * d_out * (Ymax**2) * (np.exp(1) * d_in * temp)**beta * (1/beta)**beta\n",
    "    return bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pom = lambda C,alpha, z : C.dot(z) + alpha\n",
    "bernoulli = lambda n,p : (np.random.rand(n) < p) + 0\n",
    "\n",
    "def erdos_renyi(n,p):\n",
    "    '''\n",
    "    Generates a random network of n nodes using the Erdos-Renyi method,\n",
    "    where the probability that an edge exists between two nodes is p.\n",
    "\n",
    "    Returns the adjacency matrix of the network as an n by n scipy array\n",
    "    '''\n",
    "    A = np.random.rand(n,n)\n",
    "    A = (A < p) + 0\n",
    "    A[range(n),range(n)] = 1   # everyone is affected by their own treatment\n",
    "    return scipy.sparse.csr_array(A)\n",
    "\n",
    "# for scipy sparse A\n",
    "def simpleWeights(A, diag=5, offdiag=5, rand_diag=np.array([]), rand_offdiag=np.array([])):\n",
    "    '''\n",
    "    Returns weights generated from simpler model\n",
    "\n",
    "    A (scipy array): adjacency matrix of the network\n",
    "    diag (float): maximum norm of direct effects\n",
    "    offidiag (float): maximum norm of the indirect effects\n",
    "    '''\n",
    "    n = A.shape[0]\n",
    "\n",
    "    if rand_offdiag.size == 0:\n",
    "        rand_offdiag = np.random.rand(n)\n",
    "    C_offdiag = offdiag*rand_offdiag\n",
    "\n",
    "    in_deg = scipy.sparse.diags(np.array(A.sum(axis=1)).flatten(),0)  # array of the in-degree of each node\n",
    "    C = in_deg.dot(A - scipy.sparse.eye(n))\n",
    "    col_sum = np.array(C.sum(axis=0)).flatten()\n",
    "    col_sum[col_sum==0] = 1\n",
    "    temp = scipy.sparse.diags(C_offdiag/col_sum)\n",
    "    C = C.dot(temp)\n",
    "\n",
    "    if rand_diag.size == 0:\n",
    "        rand_diag = np.random.rand(n)\n",
    "    C_diag = diag*rand_diag\n",
    "    C.setdiag(C_diag)\n",
    "\n",
    "    return C\n",
    "\n",
    "def est_us(n, p, y, A, z):\n",
    "    '''\n",
    "    Returns an estimate of the TTE using our proposed estimator\n",
    "\n",
    "    n (int): number of individuals\n",
    "    p (float): treatment probability\n",
    "    y (numpy array): observations\n",
    "    A (numpy or scipy array): network adjacency matrix\n",
    "    z (numpy array): treatment vector\n",
    "    '''\n",
    "    zz = z/p - (1-z)/(1-p)\n",
    "    return 1/n * y.dot(A.dot(zz))\n",
    "\n",
    "# for numpy array A\n",
    "def simpleWeights_np(A, diag=5, offdiag=5, rand_diag=np.array([]), rand_offdiag=np.array([])):\n",
    "    '''\n",
    "    Returns weights generated from simpler model\n",
    "\n",
    "    A (numpy array): adjacency matrix of the network\n",
    "    diag (float): maximum norm of direct effects\n",
    "    offidiag (float): maximum norm of the indirect effects\n",
    "    '''\n",
    "    n = A.shape[0]\n",
    "\n",
    "    if rand_offdiag.size == 0:\n",
    "        rand_offdiag = np.random.rand(n)\n",
    "    C_offdiag = offdiag*rand_offdiag\n",
    "\n",
    "    in_deg = np.diag(np.array(A.sum(axis=1)).flatten(),0)  # array of the in-degree of each node\n",
    "    C = in_deg.dot(A - np.eye(n))\n",
    "    col_sum = np.array(C.sum(axis=0)).flatten()\n",
    "    col_sum[col_sum==0] = 1\n",
    "    temp = np.diag(C_offdiag/col_sum)\n",
    "    C = C.dot(temp)\n",
    "\n",
    "    if rand_diag.size == 0:\n",
    "        rand_diag = np.random.rand(n)\n",
    "    C_diag = diag*rand_diag\n",
    "    C[range(n),range(n)] = C_diag\n",
    "\n",
    "    return C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different versions of variance estimator implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTE: 1.5143968790448863\n",
      "\n",
      "Loop took: 3.5189237793286643 minutes to run.\n",
      "MSE (Experimental Variance): 0.14960705092819565\n",
      "Variance Bound: 2562.4923139572734\n",
      "\n",
      "Variance Estimate (new): 752.6014893873169\n",
      "Variance Estimator bias: 752.4518823363886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 7500\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(C,alpha,z)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE: {}\\n\".format(TTE))\n",
    "\n",
    "T = 10\n",
    "TTE_hat, TTE_var_hat = np.zeros(T), np.zeros(T)\n",
    "\n",
    "N = [np.nonzero(A[[i],:])[1] for i in range(n)]  # neighbors\n",
    "dep_neighbors = A.dot(A.transpose())\n",
    "M = [np.nonzero(dep_neighbors[[i],:])[1] for i in range(n)] # dependencies\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat[i] = var_est_log2(n, p, y, A, z, N, M)\n",
    "print(\"Loop took: {} minutes to run.\".format((time.time()-time1)/60))\n",
    "\n",
    "bound = var_bound(n, p, A, C, alpha)\n",
    "\n",
    "exp_var = np.sum((TTE_hat-TTE)**2)/T\n",
    "print(\"MSE (Experimental Variance): {}\".format(exp_var))\n",
    "print(\"Variance Bound: {}\\n\".format(bound))\n",
    "\n",
    "print(\"Variance Estimate (new): {}\".format(np.sum(TTE_var_hat)/T))\n",
    "print(\"Variance Estimator bias: {}\\n\".format(((np.sum(TTE_var_hat)/T) - exp_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTE1: 1.5163220983889918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayleencortez/Desktop/NetworkCausalInference/ClusterRD/low-order-clusterRD/.venv/lib/python3.11/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(C,alpha,z)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE1: {}\\n\".format(TTE))\n",
    "\n",
    "T = 10\n",
    "TTE_hat, TTE_var_hat1, TTE_var_hat2, TTE_var_hat3 = np.zeros(T), np.zeros(T), np.zeros(T), np.zeros(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Experimental Variance): 0.1813056782212066\n",
      "Variance Bound: 1775.742722168508\n",
      "\n",
      "Variance Estimate (new): 413.26817825792415\n",
      "Variance Estimator bias: 413.08687257970297\n",
      "\n",
      "Variance Estimate (with adjustment): 413.26817825792415\n",
      "Variance Estimator bias: 413.08687257970297\n",
      "\n",
      "Total time (minutes): 4.798470648129781\n"
     ]
    }
   ],
   "source": [
    "N = [np.nonzero(A[[i],:])[1] for i in range(n)]  # neighbors\n",
    "dep_neighbors = A.dot(A.transpose())\n",
    "M = [np.nonzero(dep_neighbors[[i],:])[1] for i in range(n)] # dependencies\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat2[i] = var_est_log(n, p, y, A, z)\n",
    "    TTE_var_hat3[i] = var_est_log2(n, p, y, A, z, N, M)\n",
    "\n",
    "bound = var_bound(n, p, A, C, alpha)\n",
    "\n",
    "exp_var = np.sum((TTE_hat-TTE)**2)/T\n",
    "print(\"MSE (Experimental Variance): {}\".format(exp_var))\n",
    "print(\"Variance Bound: {}\\n\".format(bound))\n",
    "\n",
    "print(\"Variance Estimate (new): {}\".format(np.sum(TTE_var_hat2)/T))\n",
    "print(\"Variance Estimator bias: {}\\n\".format(((np.sum(TTE_var_hat2)/T) - exp_var)))\n",
    "\n",
    "print(\"Variance Estimate (with adjustment): {}\".format(np.sum(TTE_var_hat3)/T))\n",
    "print(\"Variance Estimator bias: {}\\n\".format(((np.sum(TTE_var_hat3)/T) - exp_var)))\n",
    "\n",
    "print(\"Total time (minutes): {}\".format((time.time()-time1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTE1: 1.4930987946410959\n",
      "\n",
      "My original code took: 8.69225392738978 minutes to run.\n",
      "Matt's new code took: 2.4298123637835185 minutes to run.\n",
      "Matt's new code with adjustment took: 2.35362042983373 minutes to run.\n",
      "Variance Estimate (my original): 447.2034749808164\n",
      "Variance Estimate (Matt's): 453.67712260747277\n",
      "Variance Estimate (Matt's with adjustment): 490.1557171090725\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(C,alpha,z)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE1: {}\\n\".format(TTE))\n",
    "\n",
    "T = 10\n",
    "TTE_hat, TTE_var_hat1, TTE_var_hat2, TTE_var_hat3 = np.zeros(T), np.zeros(T), np.zeros(T), np.zeros(T)\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat1[i] = var_est_sp(n, p, y, A, z)\n",
    "print(\"My original code took: {} minutes to run.\".format((time.time()-time1)/60))\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat2[i] = var_est_log(n, p, y, A, z)\n",
    "print(\"Matt's new code took: {} minutes to run.\".format((time.time()-time1)/60))\n",
    "\n",
    "time1 = time.time()\n",
    "N = [np.nonzero(A[[i],:])[1] for i in range(n)]  # neighbors\n",
    "dep_neighbors = A.dot(A.transpose())\n",
    "M = [np.nonzero(dep_neighbors[[i],:])[1] for i in range(n)] # dependencies\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat3[i] = var_est_log2(n, p, y, A, z, N, M)\n",
    "print(\"Matt's new code with adjustment took: {} minutes to run.\".format((time.time()-time1)/60))\n",
    "\n",
    "exp_var = np.sum((TTE_hat-TTE)**2)/T\n",
    "print(\"Variance Estimate (my original): {}\".format(np.sum(TTE_var_hat1)/T))\n",
    "print(\"Variance Estimate (Matt's): {}\".format(np.sum(TTE_var_hat2)/T))\n",
    "print(\"Variance Estimate (Matt's with adjustment): {}\".format(np.sum(TTE_var_hat3)/T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTE1: 1.5311264185044857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayleencortez/Desktop/NetworkCausalInference/ClusterRD/low-order-clusterRD/.venv/lib/python3.11/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(C,alpha,z)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE1: {}\\n\".format(TTE))\n",
    "\n",
    "T = 100\n",
    "TTE_hat, TTE_var_hat_sp = np.zeros(T), np.zeros(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIPE: 1.541482789564857\n",
      "SNIPE bias: 0.006763890254396397\n",
      "\n",
      "MSE (Experimental Variance): 0.18657603480817006\n",
      "Variance Bound: 1249.6571388475677\n",
      "\n",
      "Variance Estimate (sp): 308.5380731814132\n",
      "Variance Estimator bias: 308.351497146605\n",
      "\n",
      "Total time (minutes): 87.46593656539918\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "for i in range(T):\n",
    "    time2 = time.time()\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat_sp[i] = var_est_sp(n, p, y, A, z)\n",
    "\n",
    "bound = var_bound(n, p, A, C, alpha)\n",
    "\n",
    "print(\"SNIPE: {}\".format(np.sum(TTE_hat)/T))\n",
    "print(\"SNIPE bias: {}\\n\".format(((np.sum(TTE_hat)/T) - TTE)/TTE))\n",
    "\n",
    "exp_var = np.sum((TTE_hat-TTE)**2)/T\n",
    "print(\"MSE (Experimental Variance): {}\".format(exp_var))\n",
    "print(\"Variance Bound: {}\\n\".format(bound))\n",
    "\n",
    "print(\"Variance Estimate (sp): {}\".format(np.sum(TTE_var_hat_sp)/T))\n",
    "print(\"Variance Estimator bias: {}\\n\".format(((np.sum(TTE_var_hat_sp)/T) - exp_var)))\n",
    "\n",
    "print(\"Total time (minutes): {}\".format((time.time()-time1)/60))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing np.dot and A.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTEsp: 1.6934272551775629\n",
      "\n",
      "Ground-Truth TTEnp: 1.6934272551775629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 5\n",
    "Asp = erdos_renyi(n,deg/n)\n",
    "Anp = Asp.toarray()\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "Csp = simpleWeights(Asp, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "Cnp = simpleWeights_np(Anp, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(Csp,alpha,z)\n",
    "fy2 = lambda z: linear_pom(Cnp,alpha,z)\n",
    "\n",
    "TTEsp = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTEsp: {}\\n\".format(TTEsp))\n",
    "\n",
    "TTEnp = 1/n * np.sum((fy2(np.ones(n)) - fy2(np.zeros(n))))\n",
    "print(\"Ground-Truth TTEnp: {}\\n\".format(TTEnp))\n",
    "\n",
    "# Experiment\n",
    "z = bernoulli(n,p)\n",
    "y = fy(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "\n",
      "np.dot and A.dot are the same thing on a scipy sparse matrix: False\n",
      "np.dot and A.dot are the same thing on a numpy matrix: True\n",
      "np.dot on dense matrix and A.dot on scipy sparse matrix are the same: [[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zz = z/p - (1-z)/(1-p)\n",
    "w_sp = Asp.dot(zz)\n",
    "w_np = Anp.dot(zz)\n",
    "print(w_sp ==  w_np)\n",
    "print('\\n')\n",
    "\n",
    "Mi_sp_npdot = np.dot(Asp,Asp.T)\n",
    "Mi_sp_spdot = Asp.dot(Asp.transpose())\n",
    "\n",
    "Mi_np_npdot = np.dot(Anp,Anp.T)\n",
    "Mi_np_spdot = Anp.dot(Anp.transpose())\n",
    "\n",
    "print(\"np.dot and A.dot are the same thing on a scipy sparse matrix: {}\".format(np.array_equal(Mi_sp_npdot.toarray(),Mi_sp_spdot.toarray())))\n",
    "print(\"np.dot and A.dot are the same thing on a numpy matrix: {}\".format(np.array_equal(Mi_np_npdot,Mi_np_spdot)))\n",
    "print(\"np.dot on dense matrix and A.dot on scipy sparse matrix are the same: {}\\n\".format(Mi_sp_spdot == Mi_np_npdot))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "SNIPE_beta = lambda n,y,w : np.sum(y*w)/n\n",
    "\n",
    "def SNIPE_weights(n, p, A, z, beta):\n",
    "  treated_neighb = A.dot(z)\n",
    "  control_neighb = A.dot(1-z)\n",
    "  W = np.zeros(n)\n",
    "  for i in range(n):\n",
    "    w = 0\n",
    "    a_lim = min(beta,int(treated_neighb[i]))\n",
    "    for a in range(a_lim+1):\n",
    "      b_lim = min(beta - a,int(control_neighb[i]))\n",
    "      for b in range(b_lim+1):\n",
    "        w = w + ((1-p)**(a+b) - (-p)**(a+b)) * p**(-a) * (p-1)**(-b) * special.binom(treated_neighb[i],a)  * special.binom(control_neighb[i],b)\n",
    "    W[i] = w\n",
    "\n",
    "  return W\n",
    "\n",
    "SNIPE_beta = lambda n,y,w : np.sum(y*w)/n\n",
    "\n",
    "def SNIPE_beta_OG(n, p, y, A, z, beta):\n",
    "  # n = z.size\n",
    "  # z = z.reshape((n,1))\n",
    "  treated_neighb = A.dot(z)\n",
    "  control_neighb = A.dot(1-z)\n",
    "  est = 0\n",
    "  for i in range(n):\n",
    "    w = 0\n",
    "    a_lim = min(beta,int(treated_neighb[i]))\n",
    "    for a in range(a_lim+1):\n",
    "      b_lim = min(beta - a,int(control_neighb[i]))\n",
    "      for b in range(b_lim+1):\n",
    "        w = w + ((1-p)**(a+b) - (-p)**(a+b)) * p**(-a) * (p-1)**(-b) * special.binom(treated_neighb[i],a)  * special.binom(control_neighb[i],b)\n",
    "    est = est + y[i]*w\n",
    "\n",
    "  return est/n\n",
    "\n",
    "# Scale down the effects of higher order terms\n",
    "a1 = 1      # for linear effects\n",
    "a2 = 1    # for quadratic effects\n",
    "a3 = 1   # for cubic effects\n",
    "a4 = 1   # for quartic effects\n",
    "\n",
    "# Define f(z)\n",
    "f_linear = lambda alpha, z, gz: alpha + a1*z\n",
    "f_quadratic = lambda alpha, z, gz: alpha + a1*z + a2*np.multiply(gz,gz)\n",
    "f_cubic = lambda alpha, z, gz: alpha + a1*z + a2*np.multiply(gz,gz) + a3*np.power(gz,3)\n",
    "f_quartic = lambda alpha, z, gz: alpha + a1*z + a2*np.multiply(gz,gz) + a3*np.power(gz,3) + a4*np.power(gz,4)\n",
    "\n",
    "def ppom(beta, C, alpha):\n",
    "  '''\n",
    "  Returns k-degree polynomial potential outcomes function fy\n",
    "  \n",
    "  f (function): must be of the form f(z) = alpha + z + a2*z^2 + a3*z^3 + ... + ak*z^k\n",
    "  C (np.array): weighted adjacency matrix\n",
    "  alpha (np.array): vector of null effects\n",
    "  '''\n",
    "  # n = C.shape[0]\n",
    "  # assert np.all(f(alpha, np.zeros(n), np.zeros(n)) == alpha), 'f(0) should equal alpha'\n",
    "  #assert np.all(np.around(f(alpha, np.ones(n)) - alpha - np.ones(n), 10) >= 0), 'f must include linear component'\n",
    "\n",
    "  if beta == 0:\n",
    "      return lambda z: alpha + a1*z\n",
    "  elif beta == 1:\n",
    "      f = f_linear\n",
    "      return lambda z: alpha + a1*C.dot(z)\n",
    "  else:\n",
    "      g = lambda z : C.dot(z) / np.array(np.sum(C,1)).flatten()\n",
    "      if beta == 2:\n",
    "          f = f_quadratic\n",
    "      elif beta == 3:\n",
    "          f = f_cubic\n",
    "      elif beta == 4:\n",
    "          f = f_quartic\n",
    "      else:\n",
    "          print(\"ERROR: invalid degree\")\n",
    "      return lambda z: f(alpha, C.dot(z), g(z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create Weighted Adjcency matrix\u001b[39;00m\n\u001b[1;32m      9\u001b[0m deg \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> 10\u001b[0m A, A_np \u001b[39m=\u001b[39m erdos_renyi(n,deg\u001b[39m/\u001b[39mn)\n\u001b[1;32m     11\u001b[0m rand_wts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(n,\u001b[39m3\u001b[39m)\n\u001b[1;32m     12\u001b[0m alpha \u001b[39m=\u001b[39m rand_wts[:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mflatten() \u001b[39m# baseline effects\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "n = 15000\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.2\n",
    "beta = 2\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A, A_np = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = ppom(beta, C, alpha)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE: {}\\n\".format(TTE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "low-order-clusterRD",
   "language": "python",
   "name": "low-order-clusterrd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
