{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variance Estimator\n",
    "Let $P_\\mathbf{x} := \\mathbb{P}(\\mathbf{z} = \\mathbf{x})$.\n",
    "Let $\\mathbf{z}^*$ be the realized treatment vector.\n",
    "Let $P^* := \\mathbb{P}(\\mathbf{z} = \\mathbf{z^*})$ and $P_S^* := \\mathbb{P}(\\mathbf{z_S} = \\mathbf{z_S^*})$ for $S\\subseteq[n]$.\n",
    "Then, the variance estimator is given by $\\widehat{\\text{Var}}(\\widehat{TTE}) = (*) + (**)$ where \n",
    "$$(*) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j \\in M_i} \\frac{1}{P_{N_i \\cup N_j}^*} \\cdot Y_i(\\mathbf{z}^*)w_i(\\mathbf{z}^*) \\cdot Y_j(\\mathbf{z}^*)w_j(\\mathbf{z}^*)\\cdot \\text{Cov}_{ij}$$\n",
    "and\n",
    "$$(**) = \\frac{1}{n^2} \\sum_{i=1}^n P_{N_i}^* \\cdot Y_i^2\\mathbf{z^*} w_i^2\\mathbf{z^*} \\cdot \\sum_{j \\in M_i} 2^{|N_j|} - 2^{|N_j\\setminus N_i|}$$\n",
    "with \n",
    "$$\\text{Cov}_{ij} := \\prod_{k \\in N_i \\cup N_j} p^{\\mathbf{z}_k^*}(1-p)^{1-\\mathbf{z}_k^*}\\left(1 - \\prod_{\\ell \\in N_i \\cap N_j} p^{\\mathbf{z}_\\ell^*}(1-p)^{1-\\mathbf{z}_\\ell^*}\\right).$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Term 1\n",
    "We compute the first term, $(*)$, as follows:\n",
    "- First, create an $n \\times 1$ array $\\mathbf{YW}$ with entry $i$ equal to $Y_i(\\mathbf{z}^*)w_i(\\mathbf{z}^*)$.\n",
    "- Next, we create an $n \\times 1$ array $\\mathbf{V}$ with entry $i$ equal to $$\\sum_{j \\in M_i} \\frac{1}{P_{N_i \\cup N_j}^*} \\cdot Y_j(\\mathbf{z}^*)w_j(\\mathbf{z}^*)\\cdot \\text{Cov}_{ij}.$$\n",
    "    -- For each $i \\in [n]$, we create the following $m_i := |M_i| \\times 1$ arrays: \n",
    "    $$\\mathbf{P}_i := \\begin{bmatrix} 1/P_{N_i \\cup N_{j_1}} \\\\ \\vdots \\\\ 1/P_{N_i \\cup N_{j_{m_i}}} \\end{bmatrix},\\ \n",
    "    \\mathbf{YW}_i = \\begin{bmatrix} Y_{j_1}(\\mathbf{z}^*)w_{j_1}(\\mathbf{z}^*) \\\\ \\vdots \\\\ Y_{j_{m_i}}(\\mathbf{z}^*)w_{j_{m_i}}(\\mathbf{z}^*) \\end{bmatrix}, \\\n",
    "    \\textbf{COV}_i := \\begin{bmatrix} \\text{Cov}_{i{j_1}} \\\\ \\vdots \\\\ \\text{Cov}_{i{j_{m_i}}} \\end{bmatrix}.$$\n",
    "\n",
    "    -- Then, entry-wise multiply these arrays to get one array, sum the entries of the resulting array, and that sum is the $i$-th entry of $\\mathbf{V}$.\n",
    "- Then, return the dot product of $\\mathbf{YW}$ and $\\mathbf{V}$ divided by $n^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Term 2\n",
    "We compute the second term, $(**)$, as follows:\n",
    "- First, compute an $n \\times 1$ array $\\mathbf{PY2W2}$ where entry $i$ equals $P_{N_i}^* Y_i^2(\\mathbf{z}^*) w_i^2(\\mathbf{z}^*).$\n",
    "- Then, compute an $n \\times 1$ array $\\mathbf{CNTS}$ where entry $i$ equals $$\\sum_{j\\in M_i} 2^{|N_j|} - 2^{|N_j \\setminus N_i|}.$$\n",
    "- Finally, take the dot product of $\\mathbf{PY2W2}$ and $\\mathbf{CNTS}$ and divide by $n^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_est(n, p, y, A, z):\n",
    "    '''\n",
    "    n : int\n",
    "        size of the population\n",
    "    p : float\n",
    "        treatment probability\n",
    "    y : numpy array\n",
    "        observations\n",
    "    A : numpy array \n",
    "        adjacency matrix where (i,j)th entry = 1 iff j's treatment affects i's outcome\n",
    "    z : numpy array\n",
    "        realized treatment assignment vector\n",
    "    '''\n",
    "    zz = z/p - (1-z)/(1-p)\n",
    "    w = A.dot(zz)\n",
    "    YW = y * w\n",
    "    YW_sq = np.square(YW)\n",
    "\n",
    "    V = np.zeros(n)\n",
    "    PY2W2 = np.zeros(n)\n",
    "    CNTS = np.zeros(n)\n",
    "\n",
    "    prob_p = np.power(np.ones(n)*p, z) \n",
    "    prob_1_minus_p = np.power(1 - np.ones(n)*p, 1 - z)\n",
    "\n",
    "    dep_neighbors = np.dot(A,A.T)\n",
    "    \n",
    "    for i in np.arange(n):\n",
    "        Ni = np.nonzero(A[[i],:])\n",
    "        #print(\"Ni: {}\".format(Ni))\n",
    "        #print(Ni[1])\n",
    "        Mi = np.nonzero(dep_neighbors[[i],:]) # dependency neighbor's indices\n",
    "        Mi = Mi[1]\n",
    "        #print(\"Mi: {}\".format(Mi))\n",
    "        #print(Mi[1])\n",
    "\n",
    "        Pi = np.zeros(len(Mi))\n",
    "        COVi = np.zeros(len(Mi))\n",
    "        sum = 0\n",
    "        for j in np.arange(len(Mi)):\n",
    "            Nj = np.nonzero(A[[Mi[j]], :])\n",
    "            Ni_or_Nj = np.union1d(Ni,Nj)\n",
    "\n",
    "            # Compute Pi\n",
    "            mult_p = prob_p[Ni_or_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_or_Nj]\n",
    "            Pi[j] = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "\n",
    "            # Compute COVi\n",
    "            Ni_and_Nj = np.intersect1d(Ni,Nj)\n",
    "            mult_p = prob_p[Ni_and_Nj]\n",
    "            mult_1_minus_p = prob_1_minus_p[Ni_and_Nj]\n",
    "            temp = np.prod(mult_p) * np.prod(mult_1_minus_p)\n",
    "            COVi[j] = Pi[j] * (1 - temp)\n",
    "\n",
    "            # Compute CNTS[i]\n",
    "            Nj_minus_Ni = np.setdiff1d(Nj,Ni)\n",
    "            sum = sum + (2**len(Nj) - 2**len(Nj_minus_Ni))\n",
    "\n",
    "\n",
    "        # Compute V\n",
    "        V[i] = np.sum(1/Pi * YW[Mi] * COVi)\n",
    "\n",
    "        # Compute PY2W2\n",
    "        mult_p = prob_p[Ni[1]]\n",
    "        mult_1_minus_p = prob_1_minus_p[Ni[1]]\n",
    "        PY2W2[i] = np.prod(mult_p) * np.prod(mult_1_minus_p) * YW_sq[i]\n",
    "\n",
    "        # Compute CNTS\n",
    "        CNTS[i] = sum\n",
    "    \n",
    "    term1 = (1/n**2) * np.dot(YW, V)\n",
    "    term2 = (1/n**2) * np.dot(PY2W2, CNTS)\n",
    "    return term1+term2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_bound(n, p, A, C, alp, beta=1):\n",
    "    '''\n",
    "    Returns the conservative upper bound on the variance of the SNIPE(beta) estimator\n",
    "\n",
    "    n (int): size of the population\n",
    "    p (float): treatment probability\n",
    "    A (scipy sparse array): adjacency matrix\n",
    "    C (scipy sparse array): weighted adjacency matrix\n",
    "    alp (numpy array): baseline effects\n",
    "    beta (int): degree of the potential outcomes model\n",
    "    '''\n",
    "    in_deg = scipy.sparse.diags(np.array(A.sum(axis=1)).flatten(),0)  # array of the in-degree of each node\n",
    "    out_deg = scipy.sparse.diags(np.array(A.sum(axis=0)).flatten(),0)  # array of the out-degree of each node\n",
    "    in_deg = in_deg.tocsr() \n",
    "    out_deg = out_deg.tocsr() \n",
    "\n",
    "    d_in = in_deg.max()\n",
    "    d_out = out_deg.max()\n",
    "    temp = max(4 * (beta**2), (1 / (p*(1-p))))\n",
    "\n",
    "    if beta == 1:\n",
    "        Ymax = np.amax(scipy.sparse.diags(np.array(C.sum(axis=1)).flatten(),0) + alp)\n",
    "    else:\n",
    "        Ymax = np.amax(1 + alp)\n",
    "\n",
    "    bound = (1/n) * d_in * d_out * (Ymax**2) * (np.exp(1) * d_in * temp)**beta * (1/beta)**beta\n",
    "    return bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pom = lambda C,alpha, z : C.dot(z) + alpha\n",
    "bernoulli = lambda n,p : (np.random.rand(n) < p) + 0\n",
    "\n",
    "def erdos_renyi(n,p):\n",
    "    '''\n",
    "    Generates a random network of n nodes using the Erdos-Renyi method,\n",
    "    where the probability that an edge exists between two nodes is p.\n",
    "\n",
    "    Returns the adjacency matrix of the network as an n by n numpy array\n",
    "    '''\n",
    "    A = np.random.rand(n,n)\n",
    "    A = (A < p) + 0\n",
    "    A[range(n),range(n)] = 1   # everyone is affected by their own treatment\n",
    "    return scipy.sparse.csr_array(A)\n",
    "\n",
    "def simpleWeights(A, diag=5, offdiag=5, rand_diag=np.array([]), rand_offdiag=np.array([])):\n",
    "    '''\n",
    "    Returns weights generated from simpler model\n",
    "\n",
    "    A (numpy array): adjacency matrix of the network\n",
    "    diag (float): maximum norm of direct effects\n",
    "    offidiag (float): maximum norm of the indirect effects\n",
    "    '''\n",
    "    n = A.shape[0]\n",
    "\n",
    "    if rand_offdiag.size == 0:\n",
    "        rand_offdiag = np.random.rand(n)\n",
    "    C_offdiag = offdiag*rand_offdiag\n",
    "\n",
    "    in_deg = scipy.sparse.diags(np.array(A.sum(axis=1)).flatten(),0)  # array of the in-degree of each node\n",
    "    C = in_deg.dot(A - scipy.sparse.eye(n))\n",
    "    col_sum = np.array(C.sum(axis=0)).flatten()\n",
    "    col_sum[col_sum==0] = 1\n",
    "    temp = scipy.sparse.diags(C_offdiag/col_sum)\n",
    "    C = C.dot(temp)\n",
    "\n",
    "    # out_deg = np.array(A.sum(axis=0)).flatten() # array of the out-degree of each node\n",
    "    # out_deg[out_deg==0] = 1\n",
    "    # temp = scipy.sparse.diags(C_offdiag/out_deg)\n",
    "    # C = A.dot(temp)\n",
    "\n",
    "    if rand_diag.size == 0:\n",
    "        rand_diag = np.random.rand(n)\n",
    "    C_diag = diag*rand_diag\n",
    "    C.setdiag(C_diag)\n",
    "\n",
    "    return C\n",
    "\n",
    "def est_us(n, p, y, A, z):\n",
    "    '''\n",
    "    Returns an estimate of the TTE using our proposed estimator\n",
    "\n",
    "    n (int): number of individuals\n",
    "    p (float): treatment probability\n",
    "    y (numpy array?): observations\n",
    "    A (square numpy array): network adjacency matrix\n",
    "    z (numpy array): treatment vector\n",
    "    '''\n",
    "    zz = z/p - (1-z)/(1-p)\n",
    "    return 1/n * y.dot(A.dot(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-Truth TTE: 1.5079827930890148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayleencortez/Desktop/NetworkCausalInference/ClusterRD/low-order-clusterRD/.venv/lib/python3.11/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "r = 2\n",
    "diag = 1\n",
    "offdiag = r*diag\n",
    "p = 0.05\n",
    "\n",
    "# Create Weighted Adjcency matrix\n",
    "deg = 10\n",
    "A = erdos_renyi(n,deg/n)\n",
    "rand_wts = np.random.rand(n,3)\n",
    "alpha = rand_wts[:,0].flatten() # baseline effects\n",
    "C = simpleWeights(A, diag, offdiag, rand_wts[:,1].flatten(), rand_wts[:,2].flatten())\n",
    "\n",
    "# Potential outcomes function\n",
    "fy = lambda z: linear_pom(C,alpha,z)\n",
    "\n",
    "TTE = 1/n * np.sum((fy(np.ones(n)) - fy(np.zeros(n))))\n",
    "print(\"Ground-Truth TTE: {}\\n\".format(TTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIPE: 1.5962803291657306\n",
      "SNIPE bias: 0.05855341087536112\n",
      "\n",
      "MSE (Experimental Variance): 0.20949495075876856\n",
      "Variance Bound: 7523.157657577504\n",
      "Variance Estimate: -2.0807634612964176\n",
      "\n",
      "Variance Estimator bias: -2.290258412055186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = 150\n",
    "TTE_hat, TTE_var_hat = np.zeros(T), np.zeros(T)\n",
    "\n",
    "for i in range(T):\n",
    "    z = bernoulli(n,p)\n",
    "    y = fy(z)\n",
    "\n",
    "    TTE_hat[i] = est_us(n, p, y, A, z)\n",
    "    TTE_var_hat[i] = var_est(n, p, y, A, z)\n",
    "\n",
    "bound = var_bound(n, p, A, C, alpha)\n",
    "\n",
    "print(\"SNIPE: {}\".format(np.sum(TTE_hat)/T))\n",
    "print(\"SNIPE bias: {}\\n\".format(((np.sum(TTE_hat)/T) - TTE)/TTE))\n",
    "\n",
    "exp_var = np.sum((TTE_hat-TTE)**2)/T\n",
    "print(\"MSE (Experimental Variance): {}\".format(exp_var))\n",
    "print(\"Variance Bound: {}\".format(bound))\n",
    "print(\"Variance Estimate: {}\\n\".format(np.sum(TTE_var_hat)/T))\n",
    "print(\"Variance Estimator bias: {}\\n\".format(((np.sum(TTE_var_hat)/T) - exp_var)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "low-order-clusterRD",
   "language": "python",
   "name": "low-order-clusterrd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
